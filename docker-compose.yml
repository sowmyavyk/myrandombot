version: '3.8'

services:
  femicase:
    build: .
    container_name: femicase
    ports:
      - "8000:8000"
    environment:
      - LLM_PROVIDER=ollama
      - LLM_MODEL=phi4-mini
      - EMBEDDING_MODEL=nomic-embed-text
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - DEBUG=false
      - RATE_LIMIT_REQUESTS=60
      - RATE_LIMIT_WINDOW=60
    volumes:
      - ./data:/app/data
    restart: unless-stopped
    extra_hosts:
      - "host.docker.internal:host-gateway"

  # Optional: Ollama service (uncomment if you want to run Ollama in Docker)
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   restart: unless-stopped

volumes:
  ollama_data:
